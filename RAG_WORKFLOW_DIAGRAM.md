# ðŸ”„ RAG Workflow Diagram

## ðŸ“‹ Complete RAG Process Flow

```mermaid
graph TD
    A[ðŸ“„ User Uploads Documents] --> B[ðŸ”§ Document Processing]
    B --> C[ðŸ“ Text Extraction]
    C --> D[ðŸ§¹ Text Preprocessing]
    D --> E[âœ‚ï¸ Text Chunking]
    E --> F[ðŸ§  Embedding Generation]
    F --> G[ðŸ—„ï¸ Vector Storage in ChromaDB]
    
    H[â“ User Asks Question] --> I[ðŸ” Query Processing]
    I --> J[ðŸ§  Query Embedding]
    J --> K[ðŸ”Ž Vector Similarity Search]
    K --> L[ðŸ“š Retrieve Relevant Documents]
    L --> M[ðŸ”— Context Building]
    M --> N[ðŸ¤– LLM Response Generation]
    N --> O[ðŸ’¬ Enhanced Answer to User]
    
    G -.-> K
    L -.-> M
    
    style A fill:#e1f5fe
    style H fill:#e1f5fe
    style O fill:#c8e6c9
    style G fill:#fff3e0
    style K fill:#fff3e0
```

## ðŸ”§ Detailed Component Flow

### **Phase 1: Document Indexing**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ðŸ“„ Document Indexing                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. ðŸ“ Load Documents (PDF, TXT, DOCX, MD, HTML)            â”‚
â”‚ 2. ðŸ“ Extract Text Content                                 â”‚
â”‚ 3. ðŸ§¹ Preprocess Text (clean, normalize)                   â”‚
â”‚ 4. âœ‚ï¸  Chunk into Segments (1000 chars, 200 overlap)       â”‚
â”‚ 5. ðŸ§  Generate Embeddings (384-dim vectors)                â”‚
â”‚ 6. ðŸ—„ï¸  Store in ChromaDB with Metadata                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Phase 2: Query Processing**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    â“ Query Processing                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. ðŸ§  Convert Query to Embedding                          â”‚
â”‚ 2. ðŸ” Search Vector Database (Cosine Similarity)          â”‚
â”‚ 3. ðŸ“š Retrieve Top-K Relevant Documents                   â”‚
â”‚ 4. ðŸ”— Build Context from Retrieved Documents              â”‚
â”‚ 5. ðŸ¤– Inject Context into LLM Prompt                      â”‚
â”‚ 6. ðŸ’¬ Generate Enhanced Response                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸŽ¯ Key Data Transformations

### **Text â†’ Vector Transformation**
```
Input Text: "Machine learning algorithms can be supervised or unsupervised"
     â†“
Preprocessing: "machine learning algorithms can be supervised or unsupervised"
     â†“
Chunking: ["Machine learning algorithms", "can be supervised", "or unsupervised"]
     â†“
Embedding: [0.123, -0.456, 0.789, ..., 0.234] (384 dimensions)
     â†“
Storage: ChromaDB with metadata {source, category, timestamp}
```

### **Query â†’ Response Transformation**
```
User Query: "What is supervised learning?"
     â†“
Query Embedding: [0.234, -0.567, 0.890, ..., 0.345]
     â†“
Vector Search: Find similar documents in ChromaDB
     â†“
Retrieved Context: "Supervised learning requires labeled training data..."
     â†“
Enhanced Prompt: "Context: [retrieved info] Question: What is supervised learning?"
     â†“
LLM Response: "Based on the context, supervised learning is..."
```

## ðŸ” Similarity Search Process

### **Cosine Similarity Calculation**
```
Query Vector:     [0.1, 0.2, 0.3, ..., 0.4]
Document Vector:  [0.2, 0.1, 0.4, ..., 0.3]

Similarity = (0.1Ã—0.2 + 0.2Ã—0.1 + 0.3Ã—0.4 + ...) / 
             (âˆš(0.1Â²+0.2Â²+...) Ã— âˆš(0.2Â²+0.1Â²+...))

Result: 0.85 (85% similarity)
```

### **Top-K Retrieval**
```
1. Calculate similarity for all documents
2. Sort by similarity score (descending)
3. Filter by threshold (e.g., > 0.5)
4. Return top K results
```

## ðŸ“Š Performance Flow

### **Indexing Performance**
```
Documents: 100 files
Processing Time: ~30 seconds
Storage: ~50MB vectors
Memory: ~100MB peak
```

### **Query Performance**
```
Query Processing: ~50ms
Vector Search: ~20ms
Context Building: ~10ms
LLM Generation: ~2-5 seconds
Total Response: ~2-5 seconds
```

## ðŸ”„ Real-time Workflow

### **User Interaction Flow**
```
1. User types question
2. RAG system processes query
3. Retrieves relevant context
4. Generates enhanced response
5. User receives answer
6. System logs interaction
```

### **Continuous Learning**
```
1. New documents uploaded
2. Automatic re-indexing
3. Updated knowledge base
4. Improved future responses
```

---

## ðŸŽ¯ Key Benefits

âœ… **Accuracy**: Grounded in actual documents  
âœ… **Relevance**: Context-aware responses  
âœ… **Scalability**: Handles large document collections  
âœ… **Flexibility**: Works with any document format  
âœ… **Performance**: Fast retrieval and generation  
âœ… **Transparency**: Shows source documents  

---

*RAG Workflow Visualization - Local Chatbot Project* ðŸ”„âœ¨
