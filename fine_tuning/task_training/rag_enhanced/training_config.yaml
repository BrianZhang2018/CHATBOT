data_config:
  filter_quality: high
  max_length: 800
  min_length: 40
  preserve_context: true
evaluation_config:
  metrics:
  - perplexity
  - context_relevance
  - answer_accuracy
  - source_attribution
  test_split: 0.1
  validation_split: 0.2
lora_config:
  lora_alpha: 40
  lora_dropout: 0.1
  lora_r: 16  # Reduced for CPU training
  target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj  # For Qwen model
task_info:
  conversation_count: 2
  description: RAG-enhanced question answering with context
  output_directory: task_training/rag_enhanced
  task_name: rag_enhanced
training_config:
  batch_size: 1  # Reduced for CPU training
  gradient_accumulation_steps: 8  # Increased to compensate
  learning_rate: 0.0002
  num_epochs: 2  # Reduced for faster training
  warmup_steps: 50  # Reduced for faster training
  fp16: false  # Disable for CPU
  dataloader_pin_memory: false  # Disable for CPU
